{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "dataset = load_dataset('ag_news')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=[]\n",
    "label=[]\n",
    "for row in dataset['train']['text']+dataset['test']['text']:\n",
    "    text.append(wordpunct_tokenize(row.lower()))\n",
    "for row in dataset['train']['label']+dataset['test']['label']:\n",
    "    label.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict={'PADDING':0}\n",
    "for sent in text:    \n",
    "    for token in sent:        \n",
    "        if token not in word_dict:\n",
    "            word_dict[token]=len(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENT_LENGTH=256\n",
    "\n",
    "news_words = []\n",
    "for sent in text:       \n",
    "    sample=[]\n",
    "    for token in sent:     \n",
    "        sample.append(word_dict[token])\n",
    "    sample = sample[:MAX_SENT_LENGTH]\n",
    "    news_words.append(sample+[0]*(MAX_SENT_LENGTH-len(sample)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "news_words=np.array(news_words,dtype='int32') \n",
    "label=np.array(label,dtype='int32') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "index=np.arange(len(label))\n",
    "train_index=index[:120000]\n",
    "np.random.shuffle(train_index)\n",
    "test_index=index[120000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/user/anaconda3/envs/wuch15/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/user/anaconda3/envs/wuch15/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/user/anaconda3/envs/wuch15/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/user/anaconda3/envs/wuch15/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/user/anaconda3/envs/wuch15/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/user/anaconda3/envs/wuch15/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import *\n",
    "from keras.models import Model, load_model\n",
    "from keras import backend as K\n",
    "from sklearn.metrics import *\n",
    "from keras.optimizers import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "news_words=np.array(news_words,dtype='int32') \n",
    "label=np.array(label,dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "index=np.arange(len(label))\n",
    "train_index=index[:120000]\n",
    "test_index=index[120000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Fastformer(Layer):\n",
    "\n",
    "    def __init__(self, nb_head, size_per_head, **kwargs):\n",
    "        self.nb_head = nb_head\n",
    "        self.size_per_head = size_per_head\n",
    "        self.output_dim = nb_head*size_per_head\n",
    "        self.now_input_shape=None\n",
    "        super(Fastformer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.now_input_shape=input_shape\n",
    "        self.WQ = self.add_weight(name='WQ', \n",
    "                                  shape=(input_shape[0][-1], self.output_dim),\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True)\n",
    "        self.WK = self.add_weight(name='WK', \n",
    "                                  shape=(input_shape[1][-1], self.output_dim),\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True) \n",
    "        self.Wq = self.add_weight(name='Wq', \n",
    "                                  shape=(self.output_dim,self.nb_head),\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True)\n",
    "        self.Wk = self.add_weight(name='Wk', \n",
    "                                  shape=(self.output_dim,self.nb_head),\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True)\n",
    "        \n",
    "        self.WP = self.add_weight(name='WP', \n",
    "                                  shape=(self.output_dim,self.output_dim),\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True)\n",
    "        \n",
    "        \n",
    "        super(Fastformer, self).build(input_shape)\n",
    "        \n",
    "    def call(self, x):\n",
    "        if len(x) == 2:\n",
    "            Q_seq,K_seq = x\n",
    "        elif len(x) == 4:\n",
    "            Q_seq,K_seq,Q_mask,K_mask = x #different mask lengths, reserved for cross attention\n",
    "\n",
    "        Q_seq = K.dot(Q_seq, self.WQ)        \n",
    "        Q_seq_reshape = K.reshape(Q_seq, (-1, self.now_input_shape[0][1], self.nb_head*self.size_per_head))\n",
    "\n",
    "        Q_att=  K.permute_dimensions(K.dot(Q_seq_reshape, self.Wq),(0,2,1))/ self.size_per_head**0.5\n",
    "\n",
    "        if len(x)  == 4:\n",
    "            Q_att = Q_att-(1-K.expand_dims(Q_mask,axis=1))*1e8\n",
    "\n",
    "        Q_att = K.softmax(Q_att)\n",
    "        Q_seq = K.reshape(Q_seq, (-1,self.now_input_shape[0][1], self.nb_head, self.size_per_head))\n",
    "        Q_seq = K.permute_dimensions(Q_seq, (0,2,1,3))\n",
    "        \n",
    "        K_seq = K.dot(K_seq, self.WK)\n",
    "        K_seq = K.reshape(K_seq, (-1,self.now_input_shape[1][1], self.nb_head, self.size_per_head))\n",
    "        K_seq = K.permute_dimensions(K_seq, (0,2,1,3))\n",
    "\n",
    "        Q_att = Lambda(lambda x: K.repeat_elements(K.expand_dims(x,axis=3),self.size_per_head,axis=3))(Q_att)\n",
    "        global_q = K.sum(multiply([Q_att, Q_seq]),axis=2)\n",
    "        \n",
    "        global_q_repeat = Lambda(lambda x: K.repeat_elements(K.expand_dims(x,axis=2), self.now_input_shape[1][1],axis=2))(global_q)\n",
    "\n",
    "        QK_interaction = multiply([K_seq, global_q_repeat])\n",
    "        QK_interaction_reshape = K.reshape(QK_interaction, (-1, self.now_input_shape[0][1], self.nb_head*self.size_per_head))\n",
    "        K_att = K.permute_dimensions(K.dot(QK_interaction_reshape, self.Wk),(0,2,1))/ self.size_per_head**0.5\n",
    "        \n",
    "        if len(x)  == 4:\n",
    "            K_att = K_att-(1-K.expand_dims(K_mask,axis=1))*1e8\n",
    "            \n",
    "        K_att = K.softmax(K_att)\n",
    "\n",
    "        K_att = Lambda(lambda x: K.repeat_elements(K.expand_dims(x,axis=3),self.size_per_head,axis=3))(K_att)\n",
    "\n",
    "        global_k = K.sum(multiply([K_att, QK_interaction]),axis=2)\n",
    "     \n",
    "        global_k_repeat = Lambda(lambda x: K.repeat_elements(K.expand_dims(x,axis=2), self.now_input_shape[0][1],axis=2))(global_k)\n",
    "        #Q=V\n",
    "        QKQ_interaction = multiply([global_k_repeat, Q_seq])\n",
    "        QKQ_interaction = K.permute_dimensions(QKQ_interaction, (0,2,1,3))\n",
    "        QKQ_interaction = K.reshape(QKQ_interaction, (-1,self.now_input_shape[0][1], self.nb_head*self.size_per_head))\n",
    "        QKQ_interaction = K.dot(QKQ_interaction, self.WP)\n",
    "        QKQ_interaction = K.reshape(QKQ_interaction, (-1,self.now_input_shape[0][1], self.nb_head,self.size_per_head))\n",
    "        QKQ_interaction = K.permute_dimensions(QKQ_interaction, (0,2,1,3))\n",
    "        QKQ_interaction = QKQ_interaction+Q_seq\n",
    "        QKQ_interaction = K.permute_dimensions(QKQ_interaction, (0,2,1,3))\n",
    "        QKQ_interaction = K.reshape(QKQ_interaction, (-1,self.now_input_shape[0][1], self.nb_head*self.size_per_head))\n",
    "\n",
    "        #many operations can be optimized if higher versions are used. \n",
    "        \n",
    "        return QKQ_interaction\n",
    "        \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0][0], input_shape[0][1], self.output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session() \n",
    "\n",
    "text_input = Input(shape=(MAX_SENT_LENGTH,), dtype='int32')\n",
    "qmask=Lambda(lambda x:  K.cast(K.cast(x,'bool'),'float32'))(text_input)\n",
    "word_emb = Embedding(len(word_dict),256, trainable=True)(text_input)\n",
    "\n",
    "#pos_emb = Embedding(MAX_SENT_LENGTH, 256, trainable=True)(Lambda(lambda x:K.zeros_like(x,dtype='int32')+K.arange(x.shape[1]))(text_input))\n",
    "#word_emb  =add([word_emb ,word_emb])\n",
    "#We find that position embedding is not important on this dataset and we removed it for simplicity. If needed, please uncomment the two lines above\n",
    "\n",
    "word_emb=Dropout(0.2)(word_emb)\n",
    "\n",
    "hidden_word_emb = Fastformer(16,16)([word_emb,word_emb,qmask,qmask])\n",
    "hidden_word_emb = Dropout(0.2)(hidden_word_emb)\n",
    "hidden_word_emb = LayerNormalization()(add([word_emb,hidden_word_emb])) \n",
    "#if there is no layer norm in old version, please import an external layernorm class from a higher version.\n",
    "\n",
    "hidden_word_emb_layer2 = Fastformer(16,16)([hidden_word_emb,hidden_word_emb,qmask,qmask])\n",
    "hidden_word_emb_layer2 = Dropout(0.2)(hidden_word_emb_layer2)\n",
    "hidden_word_emb_layer2 = LayerNormalization()(add([hidden_word_emb,hidden_word_emb_layer2]))\n",
    "\n",
    "#without FFNN for simplicity\n",
    "\n",
    "word_att = Flatten()(Dense(1)(hidden_word_emb_layer2))\n",
    "word_att = Activation('softmax')(word_att)\n",
    "text_emb = Dot((1, 1))([hidden_word_emb_layer2 , word_att])\n",
    "classifier = Dense(4, activation='softmax')(text_emb)\n",
    "                                      \n",
    "model = Model([text_input], [classifier])\n",
    "model.compile(loss=['categorical_crossentropy'],optimizer=Adam(lr=0.001), metrics=['acc'])\n",
    "\n",
    "for i in range(1):\n",
    "    model.fit(news_words[train_index],to_categorical(label)[train_index],shuffle=True,batch_size=64, epochs=1,verbose=1)\n",
    "\n",
    "\n",
    "    y_pred = model.predict([news_words[test_index] ], batch_size=128, verbose=1)\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "    y_true = label[test_index]\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    report = f1_score(y_true, y_pred, average='macro')  \n",
    "    print(acc)\n",
    "    print(report)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
